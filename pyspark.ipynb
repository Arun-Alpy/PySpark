{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arun-Alpy/PySpark/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linux Basic"
      ],
      "metadata": {
        "id": "986lu8g8r2WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61q37dhRr8Jk",
        "outputId": "f48240e0-97b8-4b6e-a029-fa51a81668a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "git version 2.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"PySpark\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4dZh976rb5D",
        "outputId": "191f2d86-8a66-4f9c-dcc0-f39f2bc84d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whoami"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L_VoRhTsH23",
        "outputId": "7c419625-3186-4e8b-fe38-13b551b7489d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcBeT-HysI9J",
        "outputId": "87315690-4537-4582-f56a-d3623187e2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Basics"
      ],
      "metadata": {
        "id": "uDeYUAqDt3sO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQEiYZm-t75M",
        "outputId": "2b19e863-c741-4489-f1ec-14bc7c271702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfHl43rBt-YQ",
        "outputId": "57b30b88-12a3-4cfa-9a50-5159c4341488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pyspark\n",
            "Version: 3.5.1\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: dataproc-spark-connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "VyuNOF37uE6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder.appName('Basics').getOrCreate()"
      ],
      "metadata": {
        "id": "zQv93_studLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataframe\n",
        "data=[(\"Hello\",\"World\")]\n",
        "columns=[\"Word1\",\"Word2\"]\n",
        "df=spark.createDataFrame(data,columns)"
      ],
      "metadata": {
        "id": "KWkfIN3AvEyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFokBlmzvoBp",
        "outputId": "dbfc5cfb-a083-45f2-c730-f58c031ee2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|Word1|Word2|\n",
            "+-----+-----+\n",
            "|Hello|World|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Transformations and Actions"
      ],
      "metadata": {
        "id": "A8s0vOp4waQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns=[\"Name\",\"Department\",\"Salary\"]\n",
        "data = [\n",
        "    (\"John\", \"Sales\", 3000),\n",
        "    (\"Jane\", \"Finance\", 4000),\n",
        "    (\"Mike\", \"Sales\", 3500),\n",
        "    (\"Alice\", \"Finance\", 3800),\n",
        "    (\"Bob\", \"IT\", 4500)\n",
        "]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdiHpY4FweBa",
        "outputId": "11e79ff6-46fd-4735-b942-0222408b372b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter:employee with salary >3500\n",
        "df_filtered=df.filter(df.Salary>3500)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J7NCVcKwvHc",
        "outputId": "928c2d71-6cc2-4a4e-873e-171b00281185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| Jane|   Finance|  4000|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#group by and aggregate: Avg salary by Department\n",
        "df_grouped=df.groupBy(\"Department\").agg({\"Salary\":\"avg\"})\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG0l8lHYyD16",
        "outputId": "87dc1f65-0239-40b3-997f-5a3dbec4c63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3250.0|\n",
            "|   Finance|     3900.0|\n",
            "|        IT|     4500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#group by and aggregate: Avg salary by Department\n",
        "df_grouped=df.groupBy(\"Department\").avg(\"Salary\")\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Y0hXWNy2UZ",
        "outputId": "9ad114c6-a8bb-49d3-e9f3-6c78e2f4b07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3250.0|\n",
            "|   Finance|     3900.0|\n",
            "|        IT|     4500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add a new column: Salary with bonus (10%)\n",
        "from pyspark.sql.functions import col\n",
        "exp=col(\"Salary\")*1.1\n",
        "df_with_bonus=df.withColumn(\"Salary_10%_bonus\",exp)\n",
        "df_with_bonus.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_gPNwWozHXr",
        "outputId": "04b89cb1-491a-4474-b564-cdc9552aa666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+------------------+\n",
            "| Name|Department|Salary|  Salary_10%_bonus|\n",
            "+-----+----------+------+------------------+\n",
            "| John|     Sales|  3000|3300.0000000000005|\n",
            "| Jane|   Finance|  4000|            4400.0|\n",
            "| Mike|     Sales|  3500|3850.0000000000005|\n",
            "|Alice|   Finance|  3800|            4180.0|\n",
            "|  Bob|        IT|  4500|            4950.0|\n",
            "+-----+----------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col,upper,lower,concat_ws,length,when\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flouxXdC0qtH",
        "outputId": "35feb05f-0bff-47b7-90c0-b5c18b0df635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change case transformation\n",
        "df_upper=df.withColumn(\"Name_upper\",upper(col(\"Name\")))\n",
        "df_lower=df.withColumn(\"Name_lower\",lower(col(\"Name\")))\n",
        "df_upper.show()\n",
        "df_lower.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xxyx1ye2SOQ",
        "outputId": "3b0e804a-3101-4cfd-b020-ee4ace80d4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+----------+\n",
            "| Name|Department|Salary|Name_upper|\n",
            "+-----+----------+------+----------+\n",
            "| John|     Sales|  3000|      JOHN|\n",
            "| Jane|   Finance|  4000|      JANE|\n",
            "| Mike|     Sales|  3500|      MIKE|\n",
            "|Alice|   Finance|  3800|     ALICE|\n",
            "|  Bob|        IT|  4500|       BOB|\n",
            "+-----+----------+------+----------+\n",
            "\n",
            "+-----+----------+------+----------+\n",
            "| Name|Department|Salary|Name_lower|\n",
            "+-----+----------+------+----------+\n",
            "| John|     Sales|  3000|      john|\n",
            "| Jane|   Finance|  4000|      jane|\n",
            "| Mike|     Sales|  3500|      mike|\n",
            "|Alice|   Finance|  3800|     alice|\n",
            "|  Bob|        IT|  4500|       bob|\n",
            "+-----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenate columns\n",
        "df_concat=df.withColumn(\"Full_Info\",concat_ws(\",\",\"Name\",\"Department\"))\n",
        "df_concat.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCIfAJ2a2o1C",
        "outputId": "6f7bcf50-5830-4e61-f2df-63749389111b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+-------------+\n",
            "| Name|Department|Salary|    Full_Info|\n",
            "+-----+----------+------+-------------+\n",
            "| John|     Sales|  3000|   John,Sales|\n",
            "| Jane|   Finance|  4000| Jane,Finance|\n",
            "| Mike|     Sales|  3500|   Mike,Sales|\n",
            "|Alice|   Finance|  3800|Alice,Finance|\n",
            "|  Bob|        IT|  4500|       Bob,IT|\n",
            "+-----+----------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# string length of names in new DF\n",
        "df_length=df.withColumn(\"Name_length\",length(col(\"Name\")))\n",
        "df_length.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JBUwF8-3T9T",
        "outputId": "ce2c4f4d-815a-4333-bbc1-e0abae5dfd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+-----------+\n",
            "| Name|Department|Salary|Name_length|\n",
            "+-----+----------+------+-----------+\n",
            "| John|     Sales|  3000|          4|\n",
            "| Jane|   Finance|  4000|          4|\n",
            "| Mike|     Sales|  3500|          4|\n",
            "|Alice|   Finance|  3800|          5|\n",
            "|  Bob|        IT|  4500|          3|\n",
            "+-----+----------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#conditional Columns(Salary category)\n",
        "df_conditional=df.withColumn(\"Salary_Category\",\n",
        "                              when(col(\"Salary\")>3500,\"High\")\n",
        "                              .when(col(\"Salary\")>3000,\"Medium\")\n",
        "                              .otherwise(\"Low\"))\n",
        "df_conditional.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMZAPdVP3sBG",
        "outputId": "c42be623-6ae0-4cd3-9ecf-ec50869d3414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+---------------+\n",
            "| Name|Department|Salary|Salary_Category|\n",
            "+-----+----------+------+---------------+\n",
            "| John|     Sales|  3000|            Low|\n",
            "| Jane|   Finance|  4000|           High|\n",
            "| Mike|     Sales|  3500|         Medium|\n",
            "|Alice|   Finance|  3800|           High|\n",
            "|  Bob|        IT|  4500|           High|\n",
            "+-----+----------+------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rename column (Salary to Base_Salary)\n",
        "df_renamed=df.withColumnRenamed(\"Salary\",\"Base_Salary\")\n",
        "df_renamed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLkuVmEn4hLB",
        "outputId": "52f0c992-7b00-4c04-e1bf-952040b6a883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----------+\n",
            "| Name|Department|Base_Salary|\n",
            "+-----+----------+-----------+\n",
            "| John|     Sales|       3000|\n",
            "| Jane|   Finance|       4000|\n",
            "| Mike|     Sales|       3500|\n",
            "|Alice|   Finance|       3800|\n",
            "|  Bob|        IT|       4500|\n",
            "+-----+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#group by department and count employees\n",
        "\n",
        "df.groupBy(\"Department\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tR1MeXP59YC",
        "outputId": "f20336c1-31cb-44c7-f7de-3b0bbf4e8c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|Department|count|\n",
            "+----------+-----+\n",
            "|     Sales|    2|\n",
            "|   Finance|    2|\n",
            "|        IT|    1|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").agg({\"Salary\":\"sum\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs4eajZYOx9z",
        "outputId": "18b8022d-1466-4f59-e0f9-b7302bc11b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|sum(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       6500|\n",
            "|   Finance|       7800|\n",
            "|        IT|       4500|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#group by department and calculate multiple Aggregations\n",
        "from pyspark.sql.functions import avg, max, min\n",
        "df.groupBy(\"Department\").agg(\n",
        "    avg(\"Salary\").alias(\"avg_salary\"),\n",
        "    max(\"Salary\").alias(\"max_salary\"),\n",
        "    min(\"Salary\").alias(\"min_salary\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eaOlG65PFvy",
        "outputId": "a7cbc5be-366b-40e9-c40d-359d570ff7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+----------+\n",
            "|Department|avg_salary|max_salary|min_salary|\n",
            "+----------+----------+----------+----------+\n",
            "|     Sales|    3250.0|      3500|      3000|\n",
            "|   Finance|    3900.0|      4000|      3800|\n",
            "|        IT|    4500.0|      4500|      4500|\n",
            "+----------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame for department info\n",
        "dept_data = [\n",
        "    (\"Sales\", \"Building A\"),\n",
        "    (\"Finance\", \"Building B\"),\n",
        "    (\"IT\", \"Building C\")\n",
        "]\n",
        "dept_columns = [\"Department\", \"Location\"]"
      ],
      "metadata": {
        "id": "mobEtdZ6PxBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRUvLDlLRKlw",
        "outputId": "15f08a12-f4ef-4bd4-fcf4-a9c6bcf80bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dept_df=spark.createDataFrame(dept_data,dept_columns)\n",
        "dept_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYyG71NbSgZn",
        "outputId": "7b8704d9-dfdf-44fe-c667-7736fe295c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join employees with department info\n",
        "\n",
        "joined_df=df.join(dept_df,on=\"Department\",how=\"inner\")\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rJ8UFU0S_Wu",
        "outputId": "e7fe3aa6-5858-41df-bd29-c971de621137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------+----------+\n",
            "|Department| Name|Salary|  Location|\n",
            "+----------+-----+------+----------+\n",
            "|   Finance| Jane|  4000|Building B|\n",
            "|   Finance|Alice|  3800|Building B|\n",
            "|        IT|  Bob|  4500|Building C|\n",
            "|     Sales| John|  3000|Building A|\n",
            "|     Sales| Mike|  3500|Building A|\n",
            "+----------+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Employee DataFrame\n",
        "emp_data = [\n",
        "    (1, \"John\", \"Sales\", 3000),\n",
        "    (2, \"Jane\", \"Finance\", 4000),\n",
        "    (3, \"Mike\", \"Sales\", 3500),\n",
        "    (4, \"Alice\", \"HR\", 3800),\n",
        "    (5, \"Bob\", \"IT\", 4500),\n",
        "    (6, \"Sam\", \"Support\", 3200)\n",
        "]\n",
        "emp_cols = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "emp_df = spark.createDataFrame(emp_data, emp_cols)\n",
        "\n",
        "# Department DataFrame\n",
        "dept_data = [\n",
        "    (\"Sales\", \"Building A\"),\n",
        "    (\"Finance\", \"Building B\"),\n",
        "    (\"IT\", \"Building C\"),\n",
        "    (\"Admin\", \"Building D\")\n",
        "]\n",
        "dept_cols = [\"Department\", \"Location\"]\n",
        "dept_df = spark.createDataFrame(dept_data, dept_cols)\n",
        "\n",
        "# Display both\n",
        "emp_df.show()\n",
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZORxzsNmTHGo",
        "outputId": "32d4db5c-c3e6-4d08-a83d-645a5c0f5c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1| John|     Sales|  3000|\n",
            "|    2| Jane|   Finance|  4000|\n",
            "|    3| Mike|     Sales|  3500|\n",
            "|    4|Alice|        HR|  3800|\n",
            "|    5|  Bob|        IT|  4500|\n",
            "|    6|  Sam|   Support|  3200|\n",
            "+-----+-----+----------+------+\n",
            "\n",
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "|     Admin|Building D|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, avg, month\n"
      ],
      "metadata": {
        "id": "TVnInQwprVhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
        "\n"
      ],
      "metadata": {
        "id": "6Fu8MntlrrKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [\n",
        "    (1001, 101, 501, \"2025-07-10 08:23:00\", 3, 25.5),\n",
        "    (1002, 102, 502, \"2025-07-11 09:45:00\", 2, 15.0),\n",
        "    (1003, 103, 503, \"2025-07-12 10:15:00\", 1, 30.0),\n",
        "    (1004, 101, 504, \"2025-07-13 12:20:00\", 5, 25.5),\n",
        "    (1005, 105, 505, \"2025-07-14 14:35:00\", 10, 45.0),\n",
        "    (1006, 102, 506, \"2025-07-15 16:00:00\", 4, 15.0)\n",
        "]\n",
        "sales_columns = [\"sale_id\", \"product_id\", \"customer_id\", \"sale_date\", \"quantity\", \"price\"]\n",
        "sales_df = spark.createDataFrame(sales_data, sales_columns)"
      ],
      "metadata": {
        "id": "MClYQWC9rt-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Product Dataset\n",
        "product_data = [\n",
        "    (101, \"Widget A\", \"Gadgets\"),\n",
        "    (102, \"Widget B\", \"Gadgets\"),\n",
        "    (103, \"Widget C\", \"Electronics\"),\n",
        "    (104, \"Widget D\", \"Electronics\"),\n",
        "    (105, \"Widget E\", \"Home & Living\")\n",
        "]\n",
        "product_columns = [\"product_id\", \"product_name\", \"category\"]\n",
        "product_df = spark.createDataFrame(product_data, product_columns)\n"
      ],
      "metadata": {
        "id": "BaHJXsLYrxzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_data = [\n",
        "    (501, \"Alice\", \"alice@example.com\", \"2025-05-20 10:10:00\"),\n",
        "    (502, \"Bob\", \"bob@example.com\", \"2025-06-15 14:00:00\"),\n",
        "    (503, \"Charlie\", \"charlie@example.com\", \"2025-04-05 09:50:00\"),\n",
        "    (504, \"David\", \"david@example.com\", \"2025-07-01 12:25:00\"),\n",
        "    (505, \"Emma\", \"emma@example.com\", \"2025-07-10 15:30:00\"),\n",
        "    (506, \"Frank\", \"frank@example.com\", \"2025-03-23 17:00:00\")\n",
        "]\n",
        "customer_columns = [\"customer_id\", \"customer_name\", \"email\", \"join_date\"]\n",
        "customer_df = spark.createDataFrame(customer_data, customer_columns)\n"
      ],
      "metadata": {
        "id": "TLRKueDPr2uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_revenue_per_product = sales_df.withColumn(\"revenue\", col(\"quantity\") * col(\"price\")) \\\n",
        "    .groupBy(\"product_id\") \\\n",
        "    .agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "\n",
        "total_revenue_per_product.show()"
      ],
      "metadata": {
        "id": "fQGnXyRdr5FP",
        "outputId": "91fb7e9f-896b-4a19-a84c-8d9e87477bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|product_id|total_revenue|\n",
            "+----------+-------------+\n",
            "|       103|         30.0|\n",
            "|       101|        204.0|\n",
            "|       102|         90.0|\n",
            "|       105|        450.0|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_quantity_per_customer = sales_df.groupBy(\"customer_id\") \\\n",
        "    .agg(sum(\"quantity\").alias(\"total_quantity\"))\n",
        "\n",
        "total_quantity_per_customer.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "BMnzjYmBr8oD",
        "outputId": "4997747c-dcb9-453a-c9b1-ba009c5f5fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|total_quantity|\n",
            "+-----------+--------------+\n",
            "|        502|             2|\n",
            "|        501|             3|\n",
            "|        503|             1|\n",
            "|        504|             5|\n",
            "|        506|             4|\n",
            "|        505|            10|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_revenue_per_customer = sales_df.withColumn(\"revenue\", col(\"quantity\") * col(\"price\")) \\\n",
        "    .groupBy(\"customer_id\") \\\n",
        "    .agg(avg(\"revenue\").alias(\"average_revenue\"))\n",
        "\n",
        "average_revenue_per_customer.show()"
      ],
      "metadata": {
        "id": "MjlBReBBr_wb",
        "outputId": "4cdb98e3-e06d-4814-a88e-4285586fcfe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+\n",
            "|customer_id|average_revenue|\n",
            "+-----------+---------------+\n",
            "|        502|           30.0|\n",
            "|        501|           76.5|\n",
            "|        503|           30.0|\n",
            "|        504|          127.5|\n",
            "|        506|           60.0|\n",
            "|        505|          450.0|\n",
            "+-----------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_sales_total = sales_df.withColumn(\"month\", month(col(\"sale_date\"))) \\\n",
        "    .withColumn(\"revenue\", col(\"quantity\") * col(\"price\")) \\\n",
        "    .groupBy(\"month\") \\\n",
        "    .agg(sum(\"revenue\").alias(\"total_monthly_revenue\"))\n",
        "\n",
        "monthly_sales_total.show()"
      ],
      "metadata": {
        "id": "KGvakoR_sCVP",
        "outputId": "88b31395-9679-4279-a1c1-6801c48a3524",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------------+\n",
            "|month|total_monthly_revenue|\n",
            "+-----+---------------------+\n",
            "|    7|                774.0|\n",
            "+-----+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_with_category = sales_df.join(product_df, \"product_id\")\n",
        "sales_per_category = sales_with_category.groupBy(\"category\") \\\n",
        "    .count() \\\n",
        "    .withColumnRenamed(\"count\", \"total_sales\")\n",
        "\n",
        "sales_per_category.show()"
      ],
      "metadata": {
        "id": "Eqrlha5HsE3n",
        "outputId": "03162026-4270-4b4a-ca42-2ef5d9031e0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+\n",
            "|     category|total_sales|\n",
            "+-------------+-----------+\n",
            "|  Electronics|          1|\n",
            "|      Gadgets|          4|\n",
            "|Home & Living|          1|\n",
            "+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "top_3_products = sales_df.withColumn(\"revenue\", col(\"quantity\") * col(\"price\")) \\\n",
        "    .groupBy(\"product_id\") \\\n",
        "    .agg(sum(\"revenue\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(desc(\"total_revenue\")) \\\n",
        "    .limit(3)\n",
        "\n",
        "top_3_products.show()"
      ],
      "metadata": {
        "id": "8A9QWrMpsHt1",
        "outputId": "ba4d36ae-a055-4dd8-ab15-9eaaa79ed593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|product_id|total_revenue|\n",
            "+----------+-------------+\n",
            "|       105|        450.0|\n",
            "|       101|        204.0|\n",
            "|       102|         90.0|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_with_product_info = sales_df.join(product_df, \"product_id\")\n",
        "sales_with_product_info.show()"
      ],
      "metadata": {
        "id": "fl5sV2wSsKDG",
        "outputId": "b88e6808-bde1-4d7b-ff92-44fa56508434",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "|product_id|sale_id|customer_id|          sale_date|quantity|price|product_name|     category|\n",
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "|       101|   1001|        501|2025-07-10 08:23:00|       3| 25.5|    Widget A|      Gadgets|\n",
            "|       101|   1004|        504|2025-07-13 12:20:00|       5| 25.5|    Widget A|      Gadgets|\n",
            "|       102|   1002|        502|2025-07-11 09:45:00|       2| 15.0|    Widget B|      Gadgets|\n",
            "|       102|   1006|        506|2025-07-15 16:00:00|       4| 15.0|    Widget B|      Gadgets|\n",
            "|       103|   1003|        503|2025-07-12 10:15:00|       1| 30.0|    Widget C|  Electronics|\n",
            "|       105|   1005|        505|2025-07-14 14:35:00|      10| 45.0|    Widget E|Home & Living|\n",
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_with_customer_info = sales_df.join(customer_df, \"customer_id\")\n",
        "sales_with_customer_info.show()"
      ],
      "metadata": {
        "id": "FBjla7tzsR6L",
        "outputId": "99195673-f8a6-4599-d2e8-357d92a98a61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+----------+-------------------+--------+-----+-------------+-------------------+-------------------+\n",
            "|customer_id|sale_id|product_id|          sale_date|quantity|price|customer_name|              email|          join_date|\n",
            "+-----------+-------+----------+-------------------+--------+-----+-------------+-------------------+-------------------+\n",
            "|        501|   1001|       101|2025-07-10 08:23:00|       3| 25.5|        Alice|  alice@example.com|2025-05-20 10:10:00|\n",
            "|        502|   1002|       102|2025-07-11 09:45:00|       2| 15.0|          Bob|    bob@example.com|2025-06-15 14:00:00|\n",
            "|        503|   1003|       103|2025-07-12 10:15:00|       1| 30.0|      Charlie|charlie@example.com|2025-04-05 09:50:00|\n",
            "|        504|   1004|       101|2025-07-13 12:20:00|       5| 25.5|        David|  david@example.com|2025-07-01 12:25:00|\n",
            "|        505|   1005|       105|2025-07-14 14:35:00|      10| 45.0|         Emma|   emma@example.com|2025-07-10 15:30:00|\n",
            "|        506|   1006|       102|2025-07-15 16:00:00|       4| 15.0|        Frank|  frank@example.com|2025-03-23 17:00:00|\n",
            "+-----------+-------+----------+-------------------+--------+-----+-------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inner_join_gadgets = sales_df.join(product_df, \"product_id\", \"inner\") \\\n",
        "    .filter(product_df.category == \"Gadgets\")\n",
        "\n",
        "inner_join_gadgets.show()"
      ],
      "metadata": {
        "id": "tfml48-zsT_t",
        "outputId": "3c76a798-e4ce-4a66-e4d9-570a5f5cb20f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-----------+-------------------+--------+-----+------------+--------+\n",
            "|product_id|sale_id|customer_id|          sale_date|quantity|price|product_name|category|\n",
            "+----------+-------+-----------+-------------------+--------+-----+------------+--------+\n",
            "|       101|   1001|        501|2025-07-10 08:23:00|       3| 25.5|    Widget A| Gadgets|\n",
            "|       101|   1004|        504|2025-07-13 12:20:00|       5| 25.5|    Widget A| Gadgets|\n",
            "|       102|   1002|        502|2025-07-11 09:45:00|       2| 15.0|    Widget B| Gadgets|\n",
            "|       102|   1006|        506|2025-07-15 16:00:00|       4| 15.0|    Widget B| Gadgets|\n",
            "+----------+-------+-----------+-------------------+--------+-----+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "left_join_sales_product = sales_df.join(product_df, \"product_id\", \"left\")\n",
        "left_join_sales_product.show()"
      ],
      "metadata": {
        "id": "iP_itXKKsWFD",
        "outputId": "b008692d-d190-48b3-e67b-17199bd4acce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "|product_id|sale_id|customer_id|          sale_date|quantity|price|product_name|     category|\n",
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "|       103|   1003|        503|2025-07-12 10:15:00|       1| 30.0|    Widget C|  Electronics|\n",
            "|       101|   1001|        501|2025-07-10 08:23:00|       3| 25.5|    Widget A|      Gadgets|\n",
            "|       102|   1002|        502|2025-07-11 09:45:00|       2| 15.0|    Widget B|      Gadgets|\n",
            "|       105|   1005|        505|2025-07-14 14:35:00|      10| 45.0|    Widget E|Home & Living|\n",
            "|       101|   1004|        504|2025-07-13 12:20:00|       5| 25.5|    Widget A|      Gadgets|\n",
            "|       102|   1006|        506|2025-07-15 16:00:00|       4| 15.0|    Widget B|      Gadgets|\n",
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "sales_alias1 = sales_df.alias(\"s1\")\n",
        "sales_alias2 = sales_df.alias(\"s2\")\n",
        "\n",
        "self_join_df = sales_alias1.join(sales_alias2, col(\"s1.product_id\") == col(\"s2.product_id\")) \\\n",
        "    .filter(col(\"s1.sale_id\") < col(\"s2.sale_id\"))\n",
        "\n",
        "self_join_df.select(\"s1.product_id\", \"s1.sale_date\", \"s2.sale_date\").show()"
      ],
      "metadata": {
        "id": "zy-hzeNKsYE_",
        "outputId": "a840552f-ee7e-4ee1-fdec-a4028e281d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-------------------+\n",
            "|product_id|          sale_date|          sale_date|\n",
            "+----------+-------------------+-------------------+\n",
            "|       101|2025-07-10 08:23:00|2025-07-13 12:20:00|\n",
            "|       102|2025-07-11 09:45:00|2025-07-15 16:00:00|\n",
            "+----------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_outer_join_sales_product = sales_df.join(product_df, \"product_id\", \"full\")\n",
        "full_outer_join_sales_product.show()\n"
      ],
      "metadata": {
        "id": "RDq67UiFsbOw",
        "outputId": "b0f0c1e0-fef4-448c-b520-c3c980780a59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "|product_id|sale_id|customer_id|          sale_date|quantity|price|product_name|     category|\n",
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "|       101|   1001|        501|2025-07-10 08:23:00|       3| 25.5|    Widget A|      Gadgets|\n",
            "|       101|   1004|        504|2025-07-13 12:20:00|       5| 25.5|    Widget A|      Gadgets|\n",
            "|       102|   1002|        502|2025-07-11 09:45:00|       2| 15.0|    Widget B|      Gadgets|\n",
            "|       102|   1006|        506|2025-07-15 16:00:00|       4| 15.0|    Widget B|      Gadgets|\n",
            "|       103|   1003|        503|2025-07-12 10:15:00|       1| 30.0|    Widget C|  Electronics|\n",
            "|       104|   NULL|       NULL|               NULL|    NULL| NULL|    Widget D|  Electronics|\n",
            "|       105|   1005|        505|2025-07-14 14:35:00|      10| 45.0|    Widget E|Home & Living|\n",
            "+----------+-------+-----------+-------------------+--------+-----+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complete_df = sales_df.join(product_df, \"product_id\") \\\n",
        "    .join(customer_df, \"customer_id\")\n",
        "\n",
        "complete_df.show()"
      ],
      "metadata": {
        "id": "j1usbks8seET",
        "outputId": "2643bd56-da00-40f1-f8d0-68d6dd33dac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------+-------------------+--------+-----+------------+-------------+-------------+-------------------+-------------------+\n",
            "|customer_id|product_id|sale_id|          sale_date|quantity|price|product_name|     category|customer_name|              email|          join_date|\n",
            "+-----------+----------+-------+-------------------+--------+-----+------------+-------------+-------------+-------------------+-------------------+\n",
            "|        502|       102|   1002|2025-07-11 09:45:00|       2| 15.0|    Widget B|      Gadgets|          Bob|    bob@example.com|2025-06-15 14:00:00|\n",
            "|        501|       101|   1001|2025-07-10 08:23:00|       3| 25.5|    Widget A|      Gadgets|        Alice|  alice@example.com|2025-05-20 10:10:00|\n",
            "|        503|       103|   1003|2025-07-12 10:15:00|       1| 30.0|    Widget C|  Electronics|      Charlie|charlie@example.com|2025-04-05 09:50:00|\n",
            "|        504|       101|   1004|2025-07-13 12:20:00|       5| 25.5|    Widget A|      Gadgets|        David|  david@example.com|2025-07-01 12:25:00|\n",
            "|        506|       102|   1006|2025-07-15 16:00:00|       4| 15.0|    Widget B|      Gadgets|        Frank|  frank@example.com|2025-03-23 17:00:00|\n",
            "|        505|       105|   1005|2025-07-14 14:35:00|      10| 45.0|    Widget E|Home & Living|         Emma|   emma@example.com|2025-07-10 15:30:00|\n",
            "+-----------+----------+-------+-------------------+--------+-----+------------+-------------+-------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "filtered_sales_df = sales_df.filter(col(\"sale_date\").between(\"2025-07-10\", \"2025-07-15\"))\n",
        "filtered_sales_df.show()"
      ],
      "metadata": {
        "id": "ZPqJtFbAsgIu",
        "outputId": "b09a3aec-eb50-43d8-bf0a-e4ae3c7ed19f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|customer_id|          sale_date|quantity|price|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|   1001|       101|        501|2025-07-10 08:23:00|       3| 25.5|\n",
            "|   1002|       102|        502|2025-07-11 09:45:00|       2| 15.0|\n",
            "|   1003|       103|        503|2025-07-12 10:15:00|       1| 30.0|\n",
            "|   1004|       101|        504|2025-07-13 12:20:00|       5| 25.5|\n",
            "|   1005|       105|        505|2025-07-14 14:35:00|      10| 45.0|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "top_5_customers = sales_df.withColumn(\"total_spend\", col(\"quantity\") * col(\"price\")) \\\n",
        "    .join(customer_df, \"customer_id\") \\\n",
        "    .groupBy(\"customer_id\", \"customer_name\") \\\n",
        "    .agg(sum(\"total_spend\").alias(\"total_spending\")) \\\n",
        "    .orderBy(desc(\"total_spending\")) \\\n",
        "    .limit(5)\n",
        "\n",
        "top_5_customers.show()"
      ],
      "metadata": {
        "id": "nhaF2CcGsirj",
        "outputId": "87c3df9f-3441-4dd6-ef8e-d1faeec6b0d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------+\n",
            "|customer_id|customer_name|total_spending|\n",
            "+-----------+-------------+--------------+\n",
            "|        505|         Emma|         450.0|\n",
            "|        504|        David|         127.5|\n",
            "|        501|        Alice|          76.5|\n",
            "|        506|        Frank|          60.0|\n",
            "|        502|          Bob|          30.0|\n",
            "+-----------+-------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "customer_spending = sales_df.withColumn(\"total_spend\", col(\"quantity\") * col(\"price\")) \\\n",
        "    .join(customer_df, \"customer_id\") \\\n",
        "    .groupBy(\"customer_id\", \"customer_name\") \\\n",
        "    .agg(sum(\"total_spend\").alias(\"total_spending\"))\n",
        "\n",
        "categorized_customers = customer_spending.withColumn(\"spending_category\",\n",
        "    when(col(\"total_spending\") < 100, \"Low\")\n",
        "    .when((col(\"total_spending\") >= 100) & (col(\"total_spending\") <= 300), \"Medium\")\n",
        "    .otherwise(\"High\")\n",
        ")\n",
        "\n",
        "categorized_customers.show()"
      ],
      "metadata": {
        "id": "mSYkgcNlslFZ",
        "outputId": "bed9aeef-1d9e-4176-9679-a32e0a92d464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------+-----------------+\n",
            "|customer_id|customer_name|total_spending|spending_category|\n",
            "+-----------+-------------+--------------+-----------------+\n",
            "|        501|        Alice|          76.5|              Low|\n",
            "|        502|          Bob|          30.0|              Low|\n",
            "|        503|      Charlie|          30.0|              Low|\n",
            "|        504|        David|         127.5|           Medium|\n",
            "|        505|         Emma|         450.0|             High|\n",
            "|        506|        Frank|          60.0|              Low|\n",
            "+-----------+-------------+--------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "customer_purchase_dates = sales_df.groupBy(\"customer_id\") \\\n",
        "    .agg(\n",
        "        min(\"sale_date\").alias(\"first_purchase_date\"),\n",
        "        max(\"sale_date\").alias(\"last_purchase_date\")\n",
        "    )\n",
        "\n",
        "customer_purchase_dates_with_names = customer_purchase_dates.join(customer_df, \"customer_id\") \\\n",
        "    .select(\"customer_id\", \"customer_name\", \"first_purchase_date\", \"last_purchase_date\")\n",
        "\n",
        "customer_purchase_dates_with_names.show()"
      ],
      "metadata": {
        "id": "bUhe62bQsniW",
        "outputId": "63e4bae7-c344-4359-ca84-027df305c106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------------+-------------------+\n",
            "|customer_id|customer_name|first_purchase_date| last_purchase_date|\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "|        502|          Bob|2025-07-11 09:45:00|2025-07-11 09:45:00|\n",
            "|        501|        Alice|2025-07-10 08:23:00|2025-07-10 08:23:00|\n",
            "|        503|      Charlie|2025-07-12 10:15:00|2025-07-12 10:15:00|\n",
            "|        504|        David|2025-07-13 12:20:00|2025-07-13 12:20:00|\n",
            "|        506|        Frank|2025-07-15 16:00:00|2025-07-15 16:00:00|\n",
            "|        505|         Emma|2025-07-14 14:35:00|2025-07-14 14:35:00|\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max as max_\n",
        "\n",
        "# Find the most recent sale date in the entire dataset\n",
        "most_recent_date = sales_df.select(max_(\"sale_date\")).first()[0]\n",
        "print(f\"Most recent sale date: {most_recent_date}\")\n",
        "\n",
        "from pyspark.sql.functions import to_date, lit, datediff\n",
        "\n",
        "# Calculate the date 30 days before the most recent date\n",
        "thirty_days_before = sales_df.withColumn(\"date\", to_date(lit(most_recent_date))) \\\n",
        "    .selectExpr(\"date_sub(date, 30) as cutoff_date\").first().cutoff_date\n",
        "\n",
        "print(f\"30-day cutoff date: {thirty_days_before}\")\n",
        "\n",
        "# Find the last purchase date for each customer\n",
        "customer_last_purchase = sales_df.groupBy(\"customer_id\") \\\n",
        "    .agg(max_(\"sale_date\").alias(\"last_purchase_date\"))\n",
        "\n",
        "# Find customers whose last purchase was before the 30-day cutoff\n",
        "churned_customers = customer_last_purchase.filter(col(\"last_purchase_date\") < lit(thirty_days_before))\n",
        "\n",
        "churned_customers_with_names = churned_customers.join(customer_df, \"customer_id\") \\\n",
        "    .select(\"customer_id\", \"customer_name\", \"last_purchase_date\")\n",
        "\n",
        "churned_customers_with_names.show()"
      ],
      "metadata": {
        "id": "B6Dh7vAqsqRk",
        "outputId": "8f56563b-7587-4b98-ca72-ff703476aa9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most recent sale date: 2025-07-15 16:00:00\n",
            "30-day cutoff date: 2025-06-15\n",
            "+-----------+-------------+------------------+\n",
            "|customer_id|customer_name|last_purchase_date|\n",
            "+-----------+-------------+------------------+\n",
            "+-----------+-------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Rjf0xTrsvYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adv PySpark"
      ],
      "metadata": {
        "id": "XNcLh8n7ay_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", 2000, [\"math\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (2, \"Bob\", 1500, [\"english\"], {\"city\": \"SF\", \"zip\": \"94105\"}),\n",
        "    (3, \"Charlie\", 2200, [\"math\", \"history\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (4, \"David\", 1200, [\"art\"], {\"city\": \"LA\", \"zip\": \"90001\"}),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[\"id\", \"name\", \"salary\", \"subjects\", \"address\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiUYpqPia23g",
        "outputId": "553ad67e-cd80-44e5-c22f-bf73c87bb87c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+\n",
            "|id |name   |salary|subjects                |address                    |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "window_spec=Window.partitionBy(\"address.city\").orderBy(\"salary\")\n",
        "df.withColumn(\"rank\",rank().over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg8E2SOObKX0",
        "outputId": "f7667827-6bbc-4909-c3a4-e90f4417b024"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+--------------------+--------------------+----+\n",
            "| id|   name|salary|            subjects|             address|rank|\n",
            "+---+-------+------+--------------------+--------------------+----+\n",
            "|  4|  David|  1200|               [art]|{zip -> 90001, ci...|   1|\n",
            "|  1|  Alice|  2000|     [math, science]|{zip -> 10001, ci...|   1|\n",
            "|  3|Charlie|  2200|[math, history, s...|{zip -> 10001, ci...|   2|\n",
            "|  2|    Bob|  1500|           [english]|{zip -> 94105, ci...|   1|\n",
            "+---+-------+------+--------------------+--------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, row_number, rank, dense_rank, max, sum, avg\n",
        "\n",
        "# Employee Data\n",
        "data = [\n",
        "    (1, \"John\", \"Sales\", 3000),\n",
        "    (2, \"Jane\", \"Finance\", 4000),\n",
        "    (3, \"Mike\", \"Sales\", 3500),\n",
        "    (4, \"Alice\", \"Finance\", 3800),\n",
        "    (5, \"Bob\", \"IT\", 4500),\n",
        "    (6, \"Tom\", \"Sales\", 3700),\n",
        "    (7, \"Jerry\", \"Finance\", 4200),\n",
        "    (8, \"Sam\", \"IT\", 4700),\n",
        "    (9, \"Steve\", \"Sales\", 3100),\n",
        "    (10, \"Rachel\", \"IT\", 4600)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7jdgoN0baHB",
        "outputId": "5e16cdc8-936c-4241-9358-af2cb546a166"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+\n",
            "|EmpID|  Name|Department|Salary|\n",
            "+-----+------+----------+------+\n",
            "|    1|  John|     Sales|  3000|\n",
            "|    2|  Jane|   Finance|  4000|\n",
            "|    3|  Mike|     Sales|  3500|\n",
            "|    4| Alice|   Finance|  3800|\n",
            "|    5|   Bob|        IT|  4500|\n",
            "|    6|   Tom|     Sales|  3700|\n",
            "|    7| Jerry|   Finance|  4200|\n",
            "|    8|   Sam|        IT|  4700|\n",
            "|    9| Steve|     Sales|  3100|\n",
            "|   10|Rachel|        IT|  4600|\n",
            "+-----+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec=Window.partitionBy((\"Department\").orderBy(col(\"Salary\")).desc())\n",
        "df.withColumn(\"Rank\",rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "u_5FHbP9eBzi",
        "outputId": "7963e6d9-faba-41a2-f904-71d0901ef395"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'orderBy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-2705498659.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwindowSpec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Department\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Salary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rank\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindowSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'orderBy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec=Window.partitionBy(\"Department\")\n",
        "df.withColumn(\"Max_Salary\",max(\"Salary\").over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl1lV-TjeW9m",
        "outputId": "64880b3b-3c83-4cfe-fe79-b31d50b2b70d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+----------+\n",
            "|EmpID|  Name|Department|Salary|Max_Salary|\n",
            "+-----+------+----------+------+----------+\n",
            "|    2|  Jane|   Finance|  4000|      4200|\n",
            "|    4| Alice|   Finance|  3800|      4200|\n",
            "|    7| Jerry|   Finance|  4200|      4200|\n",
            "|    5|   Bob|        IT|  4500|      4700|\n",
            "|    8|   Sam|        IT|  4700|      4700|\n",
            "|   10|Rachel|        IT|  4600|      4700|\n",
            "|    1|  John|     Sales|  3000|      3700|\n",
            "|    3|  Mike|     Sales|  3500|      3700|\n",
            "|    6|   Tom|     Sales|  3700|      3700|\n",
            "|    9| Steve|     Sales|  3100|      3700|\n",
            "+-----+------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec=Window.partitionBy(\"Department\")\n",
        "df.withColumn(\"Max_Salary\",max(\"Salary\").over(windowSpec)).select(col(\"Department\"),col(\"Max_Salary\")).distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Yqd71De4t5",
        "outputId": "88ee33d6-0561-4c08-a34d-f0a06a017d65"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|Max_Salary|\n",
            "+----------+----------+\n",
            "|   Finance|      4200|\n",
            "|        IT|      4700|\n",
            "|     Sales|      3700|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec=Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding,0)\n",
        "df.withColumn(\"Cumulative_Salary\",sum(\"Salary\").over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrPkrTbxflEq",
        "outputId": "0f943fb6-918a-4182-e328-b49e983a1a18"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+-----------------+\n",
            "|EmpID|  Name|Department|Salary|Cumulative_Salary|\n",
            "+-----+------+----------+------+-----------------+\n",
            "|    4| Alice|   Finance|  3800|             3800|\n",
            "|    2|  Jane|   Finance|  4000|             7800|\n",
            "|    7| Jerry|   Finance|  4200|            12000|\n",
            "|    5|   Bob|        IT|  4500|             4500|\n",
            "|   10|Rachel|        IT|  4600|             9100|\n",
            "|    8|   Sam|        IT|  4700|            13800|\n",
            "|    1|  John|     Sales|  3000|             3000|\n",
            "|    9| Steve|     Sales|  3100|             6100|\n",
            "|    3|  Mike|     Sales|  3500|             9600|\n",
            "|    6|   Tom|     Sales|  3700|            13300|\n",
            "+-----+------+----------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", 2000, [\"math\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (2, \"Bob\", 1500, [\"english\"], {\"city\": \"SF\", \"zip\": \"94105\"}),\n",
        "    (3, \"Charlie\", 2200, [\"math\", \"history\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (4, \"David\", 1200, [\"art\"], {\"city\": \"LA\", \"zip\": \"90001\"}),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[\"id\", \"name\", \"salary\", \"subjects\", \"address\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG5P3jOb4lim",
        "outputId": "16b1e050-c81b-4bea-ad1a-9dcb6bea149f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+\n",
            "|id |name   |salary|subjects                |address                    |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark Use Case Activities\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.sql.functions import col, rank, avg, udf, pandas_udf\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "\n",
        "employees_data = [\n",
        "    (1, \"Alice\", \"HR\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500),\n",
        "    (4, \"David\", \"IT\", 4500),\n",
        "    (5, \"Eve\", \"Finance\", 5000),\n",
        "    (6, \"Frank\", \"Finance\", 4800),\n",
        "]\n",
        "\n",
        "employees_df = spark.createDataFrame(employees_data, [\"id\", \"name\", \"department\", \"salary\"])\n",
        "employees_df.show()\n",
        "\n",
        "departments_data = [\n",
        "    (\"HR\", \"New York\"),\n",
        "    (\"IT\", \"San Francisco\"),\n",
        "    (\"Finance\", \"Chicago\"),\n",
        "]\n",
        "\n",
        "departments_df = spark.createDataFrame(departments_data, [\"department\", \"location\"])\n",
        "departments_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUJ8-F5v7qku",
        "outputId": "8bbe16a8-3f11-446e-a398-41d07357099e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| id| name|department|salary|\n",
            "+---+-----+----------+------+\n",
            "|  1|Alice|        HR|  3000|\n",
            "|  2|  Bob|        IT|  4000|\n",
            "|  3|Cathy|        HR|  3500|\n",
            "|  4|David|        IT|  4500|\n",
            "|  5|  Eve|   Finance|  5000|\n",
            "|  6|Frank|   Finance|  4800|\n",
            "+---+-----+----------+------+\n",
            "\n",
            "+----------+-------------+\n",
            "|department|     location|\n",
            "+----------+-------------+\n",
            "|        HR|     New York|\n",
            "|        IT|San Francisco|\n",
            "|   Finance|      Chicago|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "# Define the window spec: partition by department and order by salary descending\n",
        "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Apply the rank function using the window spec\n",
        "ranked_employees_df = employees_df.withColumn(\"salary_rank\", rank().over(windowSpec))\n",
        "\n",
        "# Show the result\n",
        "ranked_employees_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65v3ZUl4Bwk4",
        "outputId": "12721dbc-652c-4b8a-a3ff-7a3c0448f4eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+-----------+\n",
            "| id| name|department|salary|salary_rank|\n",
            "+---+-----+----------+------+-----------+\n",
            "|  5|  Eve|   Finance|  5000|          1|\n",
            "|  6|Frank|   Finance|  4800|          2|\n",
            "|  3|Cathy|        HR|  3500|          1|\n",
            "|  1|Alice|        HR|  3000|          2|\n",
            "|  4|David|        IT|  4500|          1|\n",
            "|  2|  Bob|        IT|  4000|          2|\n",
            "+---+-----+----------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Salary Per Department (Using Window)\n",
        "windowSpec=Window.partitionBy(\"department\")\n",
        "avg_salary_per_dept_df=employees_df.withColumn(\"avg_salary\",avg(\"salary\").over(windowSpec))\n",
        "avg_salary_per_dept_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AajH0kpNCgCT",
        "outputId": "bacb5e5a-7ab0-40ea-dd18-3ac107c758ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----------+\n",
            "| id| name|department|salary|avg_salary|\n",
            "+---+-----+----------+------+----------+\n",
            "|  5|  Eve|   Finance|  5000|    4900.0|\n",
            "|  6|Frank|   Finance|  4800|    4900.0|\n",
            "|  1|Alice|        HR|  3000|    3250.0|\n",
            "|  3|Cathy|        HR|  3500|    3250.0|\n",
            "|  2|  Bob|        IT|  4000|    4250.0|\n",
            "|  4|David|        IT|  4500|    4250.0|\n",
            "+---+-----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define window spec to partition by department\n",
        "windowSpec = Window.partitionBy(\"department\")\n",
        "\n",
        "# Calculate average salary using window function\n",
        "employees_with_avg_salary = employees_df.withColumn(\n",
        "    \"avg_salary_per_dept\", avg(\"salary\").over(windowSpec)\n",
        ")\n",
        "\n",
        "# Show the result\n",
        "employees_with_avg_salary.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xScwErND1yZ",
        "outputId": "db688c40-2951-4f41-b57e-bd0d5d4dce21"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+-------------------+\n",
            "| id| name|department|salary|avg_salary_per_dept|\n",
            "+---+-----+----------+------+-------------------+\n",
            "|  5|  Eve|   Finance|  5000|             4900.0|\n",
            "|  6|Frank|   Finance|  4800|             4900.0|\n",
            "|  1|Alice|        HR|  3000|             3250.0|\n",
            "|  3|Cathy|        HR|  3500|             3250.0|\n",
            "|  2|  Bob|        IT|  4000|             4250.0|\n",
            "|  4|David|        IT|  4500|             4250.0|\n",
            "+---+-----+----------+------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Salary Per Department (Using Window)\n",
        "windowSpec=Window.partitionBy(\"department\")\n",
        "avg_salary_per_dept_df=employees_df.withColumn(\"avg_salary\",avg(\"salary\").over(windowSpec))\n",
        "avg_salary_per_dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8skF157AEFXh",
        "outputId": "74129918-e643-4ff1-bfb3-ff651c3d485d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----------+\n",
            "| id| name|department|salary|avg_salary|\n",
            "+---+-----+----------+------+----------+\n",
            "|  5|  Eve|   Finance|  5000|    4900.0|\n",
            "|  6|Frank|   Finance|  4800|    4900.0|\n",
            "|  1|Alice|        HR|  3000|    3250.0|\n",
            "|  3|Cathy|        HR|  3500|    3250.0|\n",
            "|  2|  Bob|        IT|  4000|    4250.0|\n",
            "|  4|David|        IT|  4500|    4250.0|\n",
            "+---+-----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"John\", [\"Python\", \"Java\"]),\n",
        "    (\"Jane\", [\"SQL\", \"R\", \"Scala\"]),\n",
        "    (\"Mike\", [])\n",
        "]\n",
        "columns = [\"Name\", \"Skills\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61EiQmlmEQid",
        "outputId": "b72d51aa-2c78-4966-bd6d-df65c771dc32"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|Name|Skills         |\n",
            "+----+---------------+\n",
            "|John|[Python, Java] |\n",
            "|Jane|[SQL, R, Scala]|\n",
            "|Mike|[]             |\n",
            "+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_exploded=df.withColumn(\"Skill\",explode(\"Skills\"))\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAyuAjlukhR_",
        "outputId": "ece688e7-f6aa-496f-fa6c-761c688931de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+------+\n",
            "|Name|         Skills| Skill|\n",
            "+----+---------------+------+\n",
            "|John| [Python, Java]|Python|\n",
            "|John| [Python, Java]|  Java|\n",
            "|Jane|[SQL, R, Scala]|   SQL|\n",
            "|Jane|[SQL, R, Scala]|     R|\n",
            "|Jane|[SQL, R, Scala]| Scala|\n",
            "+----+---------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"people\")\n",
        "df_lateral=spark.sql(\"\"\"\n",
        "select Name,Skill from people lateral view explode(Skills) as Skill\n",
        "\"\"\")\n",
        "df_lateral.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqqhYo4LknBF",
        "outputId": "0e045cb5-e7a5-4888-af2f-d7def9f1d7b4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|Name| Skill|\n",
            "+----+------+\n",
            "|John|Python|\n",
            "|John|  Java|\n",
            "|Jane|   SQL|\n",
            "|Jane|     R|\n",
            "|Jane| Scala|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"ProductA\", \"Jan\", 100),\n",
        "    (\"ProductA\", \"Feb\", 150),\n",
        "    (\"ProductA\", \"Mar\", 120),\n",
        "    (\"ProductB\", \"Jan\", 200),\n",
        "    (\"ProductB\", \"Feb\", 230),\n",
        "    (\"ProductB\", \"Mar\", 210),\n",
        "]\n",
        "columns = [\"Product\", \"Month\", \"Sales\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taWpRg6Ll2vb",
        "outputId": "f6ffca56-79e5-49df-f999-2158443ae061"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-----+\n",
            "| Product|Month|Sales|\n",
            "+--------+-----+-----+\n",
            "|ProductA|  Jan|  100|\n",
            "|ProductA|  Feb|  150|\n",
            "|ProductA|  Mar|  120|\n",
            "|ProductB|  Jan|  200|\n",
            "|ProductB|  Feb|  230|\n",
            "|ProductB|  Mar|  210|\n",
            "+--------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df=df.groupBy(\"Product\").pivot(\"Month\").sum(\"Sales\")\n",
        "pivot_df.show()\n",
        "unpivot_df=pivot_df.selectExpr(\"Product\",\"stack(3,'Jan',Jan,'Feb',Feb,'Mar',Mar) as (Month,Sales)\")\n",
        "unpivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lquIR-HDya-A",
        "outputId": "f925cd2c-6f85-45b1-80d5-bdf9c4353d89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+---+\n",
            "| Product|Feb|Jan|Mar|\n",
            "+--------+---+---+---+\n",
            "|ProductB|230|200|210|\n",
            "|ProductA|150|100|120|\n",
            "+--------+---+---+---+\n",
            "\n",
            "+--------+-----+-----+\n",
            "| Product|Month|Sales|\n",
            "+--------+-----+-----+\n",
            "|ProductB|  Jan|  200|\n",
            "|ProductB|  Feb|  230|\n",
            "|ProductB|  Mar|  210|\n",
            "|ProductA|  Jan|  100|\n",
            "|ProductA|  Feb|  150|\n",
            "|ProductA|  Mar|  120|\n",
            "+--------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHro0uoRyfzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced PySpark For Data Engineer"
      ],
      "metadata": {
        "id": "gZ71Wj56mGuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext,SparkConf\n",
        "\n",
        "#intitlise SparkContext and SparkConf\n",
        "conf=SparkConf().setAppName(\"AdvancedOps\").setMaster(\"local\")\n",
        "sc=SparkContext(conf=conf)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "vYpGMtchnYUb",
        "outputId": "eee01735-9319-4640-cb7a-0c64937e44a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=AdvancedOps, master=local) created by __init__ at /tmp/ipython-input-2-923876238.py:5 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-4158211648.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#intitlise SparkContext and SparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AdvancedOps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=AdvancedOps, master=local) created by __init__ at /tmp/ipython-input-2-923876238.py:5 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "rdd=sc.parallelize([1,2,3,4,5])\n",
        "#transformation:Map\n",
        "rdd_mapped=rdd.map(lambda x:x*2)\n",
        "print(rdd_mapped.collect())\n",
        "\n",
        "#transformation:filter\n",
        "rdd_filtered=rdd.filter(lambda x:x%2==0)\n",
        "print(rdd_filtered.collect())\n",
        "\n",
        "#transformation:reduce\n",
        "rdd_reduced=rdd.reduce(lambda x,y:x+y)\n",
        "print(rdd_reduced)\n",
        "\n",
        "#transformation:Flat map\n",
        "rdd_flatmap=rdd.flatMap(lambda x:range(x,x+3))\n",
        "print(rdd_flatmap.collect())\n",
        "\n",
        "\n",
        "rd=sc.parallelize(\"hello\",\"world\")\n",
        "rd_flatmap=rd.flatMap(lambda x:list(x))\n",
        "print(rd_flatmap.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "_YR9uhcRmMTY",
        "outputId": "17c17358-c798-4b1a-f6b9-073e7b0172e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n",
            "[2, 4]\n",
            "15\n",
            "[1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'world'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-2788717177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd_flatmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"world\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mrd_flatmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrd_flatmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \"\"\"\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'world'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input:1,2,3,4,5\n",
        "rdd=sc.parallelize([1,2,3,4,5])\n",
        "rdd_group=rdd.groupBy(lambda x: \"even\" if x%2==0 else \"odd\")\n",
        "print([(key,list(value))for key,value in rdd_group.collect()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJy_0uoamtl5",
        "outputId": "503e3431-9516-4c83-f1f1-58964ac89400"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('odd', [1, 3, 5]), ('even', [2, 4])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"John\", \"HR\", 5000),\n",
        "    (2, \"Jane\", \"IT\", 8000),\n",
        "    (3, \"Mike\", \"IT\", 6000),\n",
        "    (4, \"Sara\", \"Finance\", 7000),\n",
        "    (5, \"David\", \"HR\", 5500)\n",
        "]\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
        "\n",
        "# Define column names\n",
        "columns = [\"ID\", \"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "# Create a DataFrame from the sample data\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGHhbwoZyJo4",
        "outputId": "f3eccd9e-5a8b-4885-b8c7-96b6b0301136"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| ID| Name|Department|Salary|\n",
            "+---+-----+----------+------+\n",
            "|  1| John|        HR|  5000|\n",
            "|  2| Jane|        IT|  8000|\n",
            "|  3| Mike|        IT|  6000|\n",
            "|  4| Sara|   Finance|  7000|\n",
            "|  5|David|        HR|  5500|\n",
            "+---+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df=df.withColumn(\"Bonus\",df[\"Salary\"]*0.1)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V-HR7uw7n9R",
        "outputId": "a001130b-2e22-4554-de3a-a80ef23818e3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+-----+\n",
            "| ID| Name|Department|Salary|Bonus|\n",
            "+---+-----+----------+------+-----+\n",
            "|  1| John|        HR|  5000|500.0|\n",
            "|  2| Jane|        IT|  8000|800.0|\n",
            "|  3| Mike|        IT|  6000|600.0|\n",
            "|  4| Sara|   Finance|  7000|700.0|\n",
            "|  5|David|        HR|  5500|550.0|\n",
            "+---+-----+----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Avg_Salary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdaDcvf-8mFu",
        "outputId": "bf014a1a-4414-4a75-d476-7d987bd0ec45"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|Avg_Salary|\n",
            "+----------+----------+\n",
            "|        HR|    5250.0|\n",
            "|   Finance|    7000.0|\n",
            "|        IT|    7000.0|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM-T7IvU9D7S",
        "outputId": "57c6df2f-0692-449a-b0af-92b3e31394a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|Department|count|\n",
            "+----------+-----+\n",
            "|        HR|    2|\n",
            "|   Finance|    1|\n",
            "|        IT|    2|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(df[\"Salary\"].desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPy2GBsI9OUi",
        "outputId": "94a96a80-12dc-4c59-d2a1-32399942b206"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+-----+\n",
            "| ID| Name|Department|Salary|Bonus|\n",
            "+---+-----+----------+------+-----+\n",
            "|  2| Jane|        IT|  8000|800.0|\n",
            "|  4| Sara|   Finance|  7000|700.0|\n",
            "|  3| Mike|        IT|  6000|600.0|\n",
            "|  5|David|        HR|  5500|550.0|\n",
            "|  1| John|        HR|  5000|500.0|\n",
            "+---+-----+----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_list=df.collect()\n",
        "for row in data_list:\n",
        "  print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex_9Cy1J9mgh",
        "outputId": "bceebf24-afd0-4dbd-ff41-7a61a6e57568"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(ID=1, Name='John', Department='HR', Salary=5000, Bonus=500.0)\n",
            "Row(ID=2, Name='Jane', Department='IT', Salary=8000, Bonus=800.0)\n",
            "Row(ID=3, Name='Mike', Department='IT', Salary=6000, Bonus=600.0)\n",
            "Row(ID=4, Name='Sara', Department='Finance', Salary=7000, Bonus=700.0)\n",
            "Row(ID=5, Name='David', Department='HR', Salary=5500, Bonus=550.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spark.sql import row\n",
        "\n",
        "\n",
        "rdd=sc.parallelize([Row(name=\"Alice\",age=25),Row(name=\"Bob\",age=30)])\n",
        "dataset=spark.createDataFrame(rdd)\n",
        "dataset.filter(lambda x:x.age>25)\n",
        "dataset.select(\"name\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "JDnNBCr7-Rwe",
        "outputId": "e7ad4803-5964-46ef-c689-dd9900274d77"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-2100328024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Alice\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Bob\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkSQLBasics\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500),\n",
        "    (4, \"David\", \"Sales\", 4500),\n",
        "    (5, \"Eva\", \"IT\", 4200)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKhb_xkwAfK4",
        "outputId": "8edd67c0-cb37-43aa-ad4c-38178424442b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "|    4|David|     Sales|  4500|\n",
            "|    5|  Eva|        IT|  4200|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to rdd\n",
        "rdd=df.rdd\n",
        "print(\"RDD example:\",rdd.map(lambda x:(x.Name,x.Salary)).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Se52MzCC0R-",
        "outputId": "614a29b9-5c4d-4644-9dc4-4353ef2f6c23"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD example: [('Alice', 3000), ('Bob', 4000), ('Cathy', 3500), ('David', 4500), ('Eva', 4200)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceGlobalTempView(\"employees\")\n",
        "strSQL=\"select * from employees\"\n",
        "sql_result=spark.sql(strSQL)\n",
        "sql_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "4qmJ_dZDDGlD",
        "outputId": "c5aff38a-a9d0-4ab3-b62f-ef9b71143c42"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `employees` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees], [], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-1474298669.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceGlobalTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"employees\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstrSQL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"select * from employees\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msql_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrSQL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msql_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `employees` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees], [], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCQPQ3mgD1dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data formats"
      ],
      "metadata": {
        "id": "jfr654InEzvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").json(\"/content/json_data\")"
      ],
      "metadata": {
        "id": "I_sige4WE17N"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/json_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvUdtX55Fias",
        "outputId": "a1f6f916-8c8c-4194-872d-3ed5f8e9a1d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-0e93f718-f88e-468c-8c48-1c66c2224983-c000.json  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/json_data/part-00000-0e93f718-f88e-468c-8c48-1c66c2224983-c000.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ccAQoBFl-u",
        "outputId": "75505d16-ff0e-4b27-844e-1e65cba053e6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"EmpID\":1,\"Name\":\"Alice\",\"Department\":\"Sales\",\"Salary\":3000}\n",
            "{\"EmpID\":2,\"Name\":\"Bob\",\"Department\":\"IT\",\"Salary\":4000}\n",
            "{\"EmpID\":3,\"Name\":\"Cathy\",\"Department\":\"HR\",\"Salary\":3500}\n",
            "{\"EmpID\":4,\"Name\":\"David\",\"Department\":\"Sales\",\"Salary\":4500}\n",
            "{\"EmpID\":5,\"Name\":\"Eva\",\"Department\":\"IT\",\"Salary\":4200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_df=spark.read.json(\"/content/json_data\")\n",
        "json_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wFGx1z1HsNn",
        "outputId": "0df69f7f-a81c-4416-bafd-a195f366b42c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------+\n",
            "|Department|EmpID| Name|Salary|\n",
            "+----------+-----+-----+------+\n",
            "|     Sales|    1|Alice|  3000|\n",
            "|        IT|    2|  Bob|  4000|\n",
            "|        HR|    3|Cathy|  3500|\n",
            "|     Sales|    4|David|  4500|\n",
            "|        IT|    5|  Eva|  4200|\n",
            "+----------+-----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am4FHn1MHy17",
        "outputId": "5a7c1ea9-e8a1-4f7c-8f7f-1e60d4c7286a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "|    4|David|     Sales|  4500|\n",
            "|    5|  Eva|        IT|  4200|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strPath=\"csv_data\"\n",
        "df.write.mode(\"overwrite\").option(\"header\",\"true\").csv(strPath)"
      ],
      "metadata": {
        "id": "EPKRUnZkH74x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_df=spark.read.option(\"header\",\"true\").csv(strPath)\n",
        "csv_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qAOcIx0IO7O",
        "outputId": "28dd08dc-60a9-44e5-bd9e-a05b42384035"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "|    4|David|     Sales|  4500|\n",
            "|    5|  Eva|        IT|  4200|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vNwkJK4mIXJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spark Streaming"
      ],
      "metadata": {
        "id": "mnuXUJJ4obSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import csv\n",
        "\n",
        "# Generate 30 records with random data\n",
        "names = [\"John\", \"Jane\", \"Mike\", \"Sara\", \"David\", \"Emily\", \"George\", \"Nina\", \"Tom\", \"Anna\"]\n",
        "departments = [\"Sales\", \"IT\", \"HR\", \"Finance\", \"Marketing\"]\n",
        "salaries = [3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
        "\n",
        "# Create and open a CSV file for writing\n",
        "with open('employee_data.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header\n",
        "    writer.writerow([\"ID\", \"Name\", \"Department\", \"Salary\"])\n",
        "\n",
        "    # Write the 30 records\n",
        "    for i in range(1, 31):\n",
        "        name = random.choice(names)\n",
        "        department = random.choice(departments)\n",
        "        salary = random.choice(salaries)\n",
        "        writer.writerow([i, name, department, salary])\n",
        "\n",
        "print(\"CSV file 'employee_data.csv' has been generated successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlGPJOdfodeg",
        "outputId": "8348243f-1c16-4b06-eba3-ab47824c62b1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'employee_data.csv' has been generated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/employee_data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PfCQPDQo2wd",
        "outputId": "6ac3d1be-f521-4a7e-d16d-5a920904f332"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID,Name,Department,Salary\r\n",
            "1,Mike,Marketing,10000\r\n",
            "2,George,HR,8000\r\n",
            "3,George,Sales,4000\r\n",
            "4,Emily,Finance,3000\r\n",
            "5,Nina,Marketing,4000\r\n",
            "6,George,Finance,5000\r\n",
            "7,David,IT,5000\r\n",
            "8,Jane,Marketing,6000\r\n",
            "9,George,HR,7000\r\n",
            "10,Emily,IT,10000\r\n",
            "11,Mike,Sales,9000\r\n",
            "12,Mike,HR,7000\r\n",
            "13,Anna,Marketing,3000\r\n",
            "14,John,Marketing,4000\r\n",
            "15,Jane,HR,4000\r\n",
            "16,George,Marketing,5000\r\n",
            "17,Nina,IT,7000\r\n",
            "18,Sara,Sales,10000\r\n",
            "19,Jane,Marketing,6000\r\n",
            "20,David,Finance,5000\r\n",
            "21,Nina,Marketing,10000\r\n",
            "22,Mike,HR,9000\r\n",
            "23,Jane,IT,7000\r\n",
            "24,John,Finance,6000\r\n",
            "25,David,HR,9000\r\n",
            "26,David,Sales,6000\r\n",
            "27,Tom,Marketing,8000\r\n",
            "28,David,Marketing,10000\r\n",
            "29,George,Marketing,4000\r\n",
            "30,Mike,HR,10000\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/ -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRLvO-Mjo65_",
        "outputId": "359fb341-6009-4a62-e6e5-2b71ecb7708b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".   .config   employee_data.csv  sample_data\n",
            "..  csv_data  json_data\t\t spark-warehouse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/empdata/"
      ],
      "metadata": {
        "id": "zu1rJlhDpMZN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/employee_data.csv /content/empdata/"
      ],
      "metadata": {
        "id": "UgkKLKY0pQTX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types as T\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "DqlVnYhapTEL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"sparkStraming\").getOrCreate()"
      ],
      "metadata": {
        "id": "fWRChOunt29C"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# schema = T.StructType() \\\n",
        "#     .add(\"EmpID\", T.IntegerType()) \\\n",
        "#     .add(\"Name\", T.StringType()) \\\n",
        "#     .add(\"Department\", T.StringType()) \\\n",
        "#     .add(\"Salary\", T.FloatType())\n",
        "\n",
        "\n",
        "# Schema Generation\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"EmpID\", T.IntegerType()),\n",
        "    T.StructField(\"Name\", T.StringType()),\n",
        "    T.StructField(\"Department\", T.StringType()),\n",
        "    T.StructField(\"Salary\", T.FloatType())\n",
        "])"
      ],
      "metadata": {
        "id": "ippa7p2xuPVt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream_df = spark.readStream \\\n",
        "    .option(\"sep\", ',') \\\n",
        "    .schema(schema) \\\n",
        "    .csv(\"/content/empdata\")"
      ],
      "metadata": {
        "id": "JaYO25BluSUj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_df = stream_df.withColumn(\"Name_upper\", F.upper(F.col(\"Name\")))"
      ],
      "metadata": {
        "id": "Fc05UF4cuUgL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = transformed_df \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "lRq1WAqduWch"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "pxB92NBCuYu0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/empdata/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wCJEThBurt_",
        "outputId": "e1e2cc31-947e-474b-ad47-bd963f2f90ae"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "employee_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat<<EOF > /content/empdata/employee_data2.csv\n",
        "1,John,Sales,3000\n",
        "2,Jane,IT,4000\n",
        "3,Mike,Sales,5000\n",
        "4,Sara,Finance,6000\n",
        "5,David,HR,7000\n",
        "6,Emily,Marketing,6000\n",
        "7,George,HR,4000\n",
        "8,Nina,Sales,5000\n",
        "9,Tom,IT,8000\n",
        "10,Anna,Marketing,3000\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "3ee8h43Ruu3X"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/empdata/employee_data2.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMZWr2CGvFFS",
        "outputId": "bced2ca3-4db7-46fa-9530-61e4a0447b2f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1,John,Sales,3000\n",
            "2,Jane,IT,4000\n",
            "3,Mike,Sales,5000\n",
            "4,Sara,Finance,6000\n",
            "5,David,HR,7000\n",
            "6,Emily,Marketing,6000\n",
            "7,George,HR,4000\n",
            "8,Nina,Sales,5000\n",
            "9,Tom,IT,8000\n",
            "10,Anna,Marketing,3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat<<EOF > /content/empdata/employee_data3.csv\n",
        "11,John,IT,7000\n",
        "12,Jane,HR,4000\n",
        "13,Mike,Finance,5000\n",
        "14,Sara,Sales,6000\n",
        "15,David,Marketing,7000\n",
        "16,Emily,Sales,8000\n",
        "17,George,Finance,3000\n",
        "18,Nina,IT,6000\n",
        "19,Tom,Sales,4000\n",
        "20,Anna,HR,5000\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "KxyiFJUXvH-d"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat<<EOF > /content/empdata/employee_data4.csv\n",
        "21,John,Finance,7000\n",
        "22,Jane,Marketing,6000\n",
        "23,Mike,HR,8000\n",
        "24,Sara,Sales,3000\n",
        "25,David,IT,6000\n",
        "26,Emily,Finance,5000\n",
        "27,George,Marketing,4000\n",
        "28,Nina,HR,7000\n",
        "29,Tom,Finance,5000\n",
        "30,Anna,IT,8000\n",
        "EOF"
      ],
      "metadata": {
        "id": "oJ1NpXqLvSFN"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/empdata/employee_data3.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnocWCrSvXme",
        "outputId": "4411c110-5210-4f25-9388-3467540be042"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11,John,IT,7000\n",
            "12,Jane,HR,4000\n",
            "13,Mike,Finance,5000\n",
            "14,Sara,Sales,6000\n",
            "15,David,Marketing,7000\n",
            "16,Emily,Sales,8000\n",
            "17,George,Finance,3000\n",
            "18,Nina,IT,6000\n",
            "19,Tom,Sales,4000\n",
            "20,Anna,HR,5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/empdata/employee_data4.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16kHgRJlvcqL",
        "outputId": "48a45787-0b41-41bf-da6f-f2d85ece46c7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21,John,Finance,7000\n",
            "22,Jane,Marketing,6000\n",
            "23,Mike,HR,8000\n",
            "24,Sara,Sales,3000\n",
            "25,David,IT,6000\n",
            "26,Emily,Finance,5000\n",
            "27,George,Marketing,4000\n",
            "28,Nina,HR,7000\n",
            "29,Tom,Finance,5000\n",
            "30,Anna,IT,8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stream_df = spark.readStream \\\n",
        "    .option(\"sep\", ',') \\\n",
        "    .schema(schema) \\\n",
        "    .csv(\"/content/empdata\")"
      ],
      "metadata": {
        "id": "yk3yohS0vePo"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_df = stream_df.withColumn(\"Name_upper\", F.upper(F.col(\"Name\")))"
      ],
      "metadata": {
        "id": "e0zDqhb3wWLl"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = transformed_df \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "_0g-0nbpwcFZ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "J1uPnoDSwfpV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYMCWg_WwkAZ",
        "outputId": "4e5f0ae1-3d8c-4e05-c1ce-1b62f48febcf"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv_data  empdata  json_data  sample_data  spark-warehouse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls empdata/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTHkhslowxUq",
        "outputId": "d5f07cf4-13df-43f7-86cc-3997a7537337"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "employee_data2.csv  employee_data3.csv\temployee_data4.csv  employee_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "\n",
        "names = [\"John\", \"Jane\", \"Mike\", \"Sara\", \"David\", \"Emily\", \"George\", \"Nina\", \"Tom\", \"Anna\"]\n",
        "departments = [\"Sales\", \"IT\", \"HR\", \"Finance\", \"Marketing\"]\n",
        "salaries = [3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
        "\n",
        "record_batch = []\n",
        "file_count = 1\n",
        "record_id = 1\n",
        "\n",
        "while True:\n",
        "    # Generate a new record\n",
        "    name = random.choice(names)\n",
        "    department = random.choice(departments)\n",
        "    salary = random.choice(salaries)\n",
        "    record = [record_id, name, department, salary]\n",
        "\n",
        "    # Add to batch\n",
        "    record_batch.append(record)\n",
        "    print(record)\n",
        "    time.sleep(1)\n",
        "    record_id += 1\n",
        "\n",
        "    # Write to file if batch has 10 records\n",
        "    if len(record_batch) == 10:\n",
        "        filename = f\"output_{file_count}.csv\"\n",
        "        with open(filename, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"ID\", \"Name\", \"Department\", \"Salary\"])\n",
        "            writer.writerows(record_batch)\n",
        "        print(f\"Written 10 records to {filename}\")\n",
        "\n",
        "        # Reset batch and increment file counter\n",
        "        record_batch = []\n",
        "        file_count += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "LZ9u7rROw2BD",
        "outputId": "06fd4514-b17d-47e8-b6cb-b4e583e673ef"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 'George', 'Marketing', 7000]\n",
            "[2, 'John', 'HR', 7000]\n",
            "[3, 'Sara', 'Sales', 9000]\n",
            "[4, 'Tom', 'Sales', 8000]\n",
            "[5, 'George', 'Marketing', 8000]\n",
            "[6, 'Sara', 'Marketing', 7000]\n",
            "[7, 'John', 'IT', 4000]\n",
            "[8, 'Jane', 'HR', 10000]\n",
            "[9, 'George', 'HR', 10000]\n",
            "[10, 'Emily', 'Finance', 10000]\n",
            "Written 10 records to output_1.csv\n",
            "[11, 'Mike', 'Finance', 8000]\n",
            "[12, 'Tom', 'IT', 7000]\n",
            "[13, 'Anna', 'IT', 3000]\n",
            "[14, 'George', 'Marketing', 7000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-69-1922515931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mrecord_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mrecord_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJCceyVb--YT",
        "outputId": "5dbbdd21-5324-406c-bcff-6b9cec5c72c2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv_data  json_data\toutput_2.csv  spark-warehouse\n",
            "empdata   output_1.csv\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat output_1.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iFb9o_iBNJk",
        "outputId": "5f9f397a-a4de-459c-cb7d-62b0b26605b0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID,Name,Department,Salary\r\n",
            "1,George,Marketing,7000\r\n",
            "2,John,HR,7000\r\n",
            "3,Sara,Sales,9000\r\n",
            "4,Tom,Sales,8000\r\n",
            "5,George,Marketing,8000\r\n",
            "6,Sara,Marketing,7000\r\n",
            "7,John,IT,4000\r\n",
            "8,Jane,HR,10000\r\n",
            "9,George,HR,10000\r\n",
            "10,Emily,Finance,10000\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i9FkYH1BQTn",
        "outputId": "c41738c4-6288-48a4-fa6c-455ffc2c4e56"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Path to your folder inside Google Drive\n",
        "folder_path = \"/content/drive/MyDrive/employee_batches\"\n",
        "os.makedirs(folder_path, exist_ok=True)  # create folder if not exist\n",
        "\n",
        "names = [\"John\", \"Jane\", \"Mike\", \"Sara\", \"David\", \"Emily\", \"George\", \"Nina\", \"Tom\", \"Anna\"]\n",
        "departments = [\"Sales\", \"IT\", \"HR\", \"Finance\", \"Marketing\"]\n",
        "salaries = [3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
        "\n",
        "record_batch = []\n",
        "file_count = 1\n",
        "record_id = 1\n",
        "\n",
        "while True:\n",
        "    # Generate random employee record\n",
        "    name = random.choice(names)\n",
        "    department = random.choice(departments)\n",
        "    salary = random.choice(salaries)\n",
        "    record = [record_id, name, department, salary]\n",
        "\n",
        "    record_batch.append(record)\n",
        "    print(record)\n",
        "    time.sleep(1)\n",
        "    record_id += 1\n",
        "\n",
        "    if len(record_batch) == 10:\n",
        "        filename = f\"output_{file_count}.csv\"\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Write CSV to Google Drive folder\n",
        "        with open(file_path, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"ID\", \"Name\", \"Department\", \"Salary\"])\n",
        "            writer.writerows(record_batch)\n",
        "\n",
        "        print(f\"Written 10 records to {file_path}\")\n",
        "\n",
        "        # Reset for next batch\n",
        "        record_batch = []\n",
        "        file_count += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "zF9m32ovCi9c",
        "outputId": "6a83a523-9a66-4f07-d5d3-a141b30502f6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 'Jane', 'Marketing', 7000]\n",
            "[2, 'David', 'Marketing', 9000]\n",
            "[3, 'Tom', 'Marketing', 3000]\n",
            "[4, 'John', 'Marketing', 4000]\n",
            "[5, 'Jane', 'HR', 4000]\n",
            "[6, 'Anna', 'Sales', 7000]\n",
            "[7, 'John', 'HR', 5000]\n",
            "[8, 'Emily', 'Finance', 6000]\n",
            "[9, 'Emily', 'Finance', 4000]\n",
            "[10, 'Anna', 'HR', 9000]\n",
            "Written 10 records to /content/drive/MyDrive/employee_batches/output_1.csv\n",
            "[11, 'Mike', 'Marketing', 5000]\n",
            "[12, 'Mike', 'Finance', 3000]\n",
            "[13, 'Mike', 'Marketing', 9000]\n",
            "[14, 'David', 'HR', 8000]\n",
            "[15, 'David', 'Marketing', 8000]\n",
            "[16, 'Nina', 'Sales', 10000]\n",
            "[17, 'Nina', 'HR', 9000]\n",
            "[18, 'David', 'HR', 9000]\n",
            "[19, 'George', 'Sales', 8000]\n",
            "[20, 'George', 'Sales', 3000]\n",
            "Written 10 records to /content/drive/MyDrive/employee_batches/output_2.csv\n",
            "[21, 'Emily', 'Marketing', 5000]\n",
            "[22, 'John', 'Marketing', 4000]\n",
            "[23, 'Mike', 'Marketing', 8000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-75-2829155848.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mrecord_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mrecord_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsfdhvOzCrwh",
        "outputId": "70ee393c-8e19-4ffb-c5fe-37a289aa2260"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "gauth.LocalWebserverAuth()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "4oawgkwkDOI3",
        "outputId": "77bbfe87-c13c-46d0-a11b-0a51d23a15f8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"color: red;\">\n",
              "pydrive is\n",
              "<a href=\"https://github.com/googlearchive/PyDrive\" target=\"_blank\">deprecated</a>\n",
              "and no longer maintained. Colab will no longer preinstall\n",
              "pydrive on 2025-07-31.\n",
              "<a href=\"https://pypi.org/project/PyDrive2\" target=\"_blank\">pydrive2</a>\n",
              "is a maintained fork of pydrive and we recommend you migrate\n",
              "your code to use pydrive2 to ensure your notebook will continue\n",
              "to function.</p>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidConfigError",
          "evalue": "Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36m_loadfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'client_secrets.json'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidClientSecretsError\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfigFile\u001b[0;34m(self, client_config_file)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m       \u001b[0mclient_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidClientSecretsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36mloadfile\u001b[0;34m(filename, cache)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_loadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/oauth2client/clientsecrets.py\u001b[0m in \u001b[0;36m_loadfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         raise InvalidClientSecretsError('Error opening file', exc.filename,\n\u001b[0m\u001b[1;32m    125\u001b[0m                                         exc.strerror, exc.errno)\n",
            "\u001b[0;31mInvalidClientSecretsError\u001b[0m: ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidConfigError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-74-2361259105.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLocalWebserverAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdrive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_import_hooks/_pydrive.py\u001b[0m in \u001b[0;36mPatchedLocalWebServerAuth\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttplib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0morig_local_webserver_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         pydrive_auth_module.GoogleAuth.LocalWebserverAuth = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadCredentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetFlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoratee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mGetFlow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m     if not all(config in self.client_config \\\n\u001b[1;32m    442\u001b[0m                for config in self.CLIENT_CONFIGS_LIST):\n\u001b[0;32m--> 443\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     constructor_kwargs = {\n\u001b[1;32m    445\u001b[0m         \u001b[0;34m'redirect_uri'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'redirect_uri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfig\u001b[0;34m(self, backend)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mInvalidConfigError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please specify client config backend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfigFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'settings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadClientConfigSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36mLoadClientConfigFile\u001b[0;34m(self, client_config_file)\u001b[0m\n\u001b[1;32m    386\u001b[0m       \u001b[0mclient_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mclientsecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidClientSecretsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mInvalidConfigError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid client secrets file %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     if not client_type in (clientsecrets.TYPE_WEB,\n\u001b[1;32m    390\u001b[0m                            clientsecrets.TYPE_INSTALLED):\n",
            "\u001b[0;31mInvalidConfigError\u001b[0m: Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3hKjYM2DVfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}